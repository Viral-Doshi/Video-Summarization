{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules/Models.py\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head=8, num_feature=1024):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.Q = nn.Linear(num_feature, num_feature, bias=False)\n",
    "        self.K = nn.Linear(num_feature, num_feature, bias=False)\n",
    "        self.V = nn.Linear(num_feature, num_feature, bias=False)\n",
    "\n",
    "        self.d_k = num_feature // num_head\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_feature, num_feature, bias=False),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, seq_len, num_feature = x.shape  # [1, seq_len, 1024]\n",
    "        K = self.K(x)  # [1, seq_len, 1024]\n",
    "        Q = self.Q(x)  # [1, seq_len, 1024]\n",
    "        V = self.V(x)  # [1, seq_len, 1024]\n",
    "\n",
    "        K = K.view(1, seq_len, self.num_head, self.d_k).permute(\n",
    "            2, 0, 1, 3).contiguous().view(self.num_head, seq_len, self.d_k)\n",
    "        Q = Q.view(1, seq_len, self.num_head, self.d_k).permute(\n",
    "            2, 0, 1, 3).contiguous().view(self.num_head, seq_len, self.d_k)\n",
    "        V = V.view(1, seq_len, self.num_head, self.d_k).permute(\n",
    "            2, 0, 1, 3).contiguous().view(self.num_head, seq_len, self.d_k)\n",
    "\n",
    "        y, attn = self.attention(Q, K, V)  # [num_head, seq_len, d_k]\n",
    "        y = y.view(1, self.num_head, seq_len, self.d_k).permute(\n",
    "            0, 2, 1, 3).contiguous().view(1, seq_len, num_feature)\n",
    "\n",
    "        y = self.fc(y)\n",
    "\n",
    "        return y, attn\n",
    "\n",
    "\n",
    "class AttentionExtractor(MultiHeadAttention):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        out, _ = super().forward(*inputs)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, seq_len, num_feature = x.shape  # [1, seq_len, 1024]\n",
    "        K = self.K(x)  # [1, seq_len, 1024]\n",
    "        Q = self.Q(x)  # [1, seq_len, 1024]\n",
    "        V = self.V(x)  # [1, seq_len, 1024]\n",
    "\n",
    "        K = K.view(1, seq_len, self.num_head, self.d_k).permute(\n",
    "            2, 0, 1, 3).contiguous().view(self.num_head, seq_len, self.d_k)\n",
    "        Q = Q.view(1, seq_len, self.num_head, self.d_k).permute(\n",
    "            2, 0, 1, 3).contiguous().view(self.num_head, seq_len, self.d_k)\n",
    "        V = V.view(1, seq_len, self.num_head, self.d_k).permute(\n",
    "            2, 0, 1, 3).contiguous().view(self.num_head, seq_len, self.d_k)\n",
    "\n",
    "        y, attn = self.attention(Q, K, V)  # [num_head, seq_len, d_k]\n",
    "        y = y.view(1, self.num_head, seq_len, self.d_k).permute(\n",
    "            0, 2, 1, 3).contiguous().view(1, seq_len, num_feature)\n",
    "\n",
    "        y = self.fc(y)\n",
    "\n",
    "        return y, attn\n",
    "\n",
    "\n",
    "\n",
    "def build_base_model(base_type, num_feature, num_head):\n",
    "    base_model = AttentionExtractor(num_head, num_feature)\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSNet(nn.Module):\n",
    "    def __init__(self, base_model, num_feature, num_hidden, anchor_scales, num_head):\n",
    "        super().__init__()\n",
    "        self.anchor_scales = anchor_scales\n",
    "        self.num_scales = len(anchor_scales)\n",
    "        self.base_model = build_base_model(base_model, num_feature, num_head)\n",
    "\n",
    "        self.roi_poolings = [nn.AvgPool1d(scale, stride=1, padding=scale // 2) for scale in anchor_scales]\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(num_feature)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(num_feature, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LayerNorm(num_hidden)\n",
    "        )\n",
    "        self.fc_cls = nn.Linear(num_hidden, 1)\n",
    "        self.fc_loc = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, seq_len, _ = x.shape\n",
    "        out = self.base_model(x)\n",
    "        out = out + x\n",
    "        out = self.layer_norm(out)\n",
    "\n",
    "        out = out.transpose(2, 1)\n",
    "        pool_results = [roi_pooling(out) for roi_pooling in self.roi_poolings]\n",
    "        out = torch.cat(pool_results, dim=0).permute(2, 0, 1)[:-1]\n",
    "\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        pred_cls = self.fc_cls(out).sigmoid().view(seq_len, self.num_scales)\n",
    "        pred_loc = self.fc_loc(out).view(seq_len, self.num_scales, 2)\n",
    "\n",
    "        return pred_cls, pred_loc\n",
    "\n",
    "    def predict(self, seq):\n",
    "        seq_len = seq.shape[1]\n",
    "        pred_cls, pred_loc = self(seq)\n",
    "\n",
    "        pred_cls = pred_cls.cpu().numpy().reshape(-1)\n",
    "        pred_loc = pred_loc.cpu().numpy().reshape((-1, 2))\n",
    "\n",
    "        anchors = anchor_helper.get_anchors(seq_len, self.anchor_scales)\n",
    "        anchors = anchors.reshape((-1, 2))\n",
    "\n",
    "        pred_bboxes = anchor_helper.offset2bbox(pred_loc, anchors)\n",
    "        pred_bboxes = bbox_helper.cw2lr(pred_bboxes)\n",
    "\n",
    "        return pred_cls, pred_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = 'anchor-based'\n",
    "        self.device = \"cuda\"\n",
    "        self.seed = 12345\n",
    "        self.splits = [\"../splits/tvsum.yml\", \"../splits/summe.yml\"]\n",
    "        self.max_epoch = 300\n",
    "        self.model_dir = [\"../models/pretrain_ab_basic/\"][0]\n",
    "        self.log_file = \"log.txt\"\n",
    "        self.lr = 5e-5\n",
    "        self.weight_decay = 1e-5\n",
    "        self.lambda_reg = 1.0\n",
    "        self.nms_thresh = 0.5\n",
    "\n",
    "        self.ckpt_path = None\n",
    "        self.sample_rate = 15\n",
    "        self.source = None\n",
    "        self.save_path = None\n",
    "\n",
    "        self.base_model = ['attention', 'lstm', 'linear', 'bilstm', 'gcn'][0]\n",
    "        self.num_head = 8\n",
    "        self.num_feature = 1024\n",
    "        self.num_hidden = 128\n",
    "\n",
    "        self.neg_sample_ratio = 2.0\n",
    "        self.incomplete_sample_ratio = 1.0\n",
    "        self.pos_iou_thresh = 0.6\n",
    "        self.neg_iou_thresh = 0.0\n",
    "        self.incomplete_iou_thresh = 0.3\n",
    "        self.anchor_scales = [4,8,16,32]\n",
    "\n",
    "        self.lambda_ctr = 1.0\n",
    "        self.cls_loss = ['focal', 'cross-entropy'][0]\n",
    "        self.reg_loss = ['soft-iou', 'smooth-l1'][0]\n",
    "\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model == 'anchor-based':\n",
    "            return DSNet(self.base_model, self.num_feature, self.num_hidden, self.anchor_scales, self.num_head)\n",
    "        elif self.model == 'anchor-free':\n",
    "            return DSNetAF(self.base_model, self.num_feature, self.num_hidden, self.num_head)\n",
    "        else:\n",
    "            print(f'Invalid model type {self.model_type}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
